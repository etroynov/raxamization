{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ramax.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVJGXBjrMxoJ"
      },
      "source": [
        "import numpy as np \n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Reshape, Flatten, Conv2D \n",
        "from keras.optimizers import Adam \n",
        "from google.colab import files \n",
        "import pandas as pd "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5mBhtwENM8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469b552d-579a-4bb9-ea9e-cb103a7c3764"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7e96FfINQYs"
      },
      "source": [
        "# Зададим глобальные параметры\n",
        "parameters = {\n",
        "    'path' : '/content/drive/My Drive/2021/R-Q-Learning/', # Путь к папке с весами\n",
        "    'observation_size' : 80 * 80, # 'Усеченные' размеры выбора ТТ\n",
        "    'action_size'  : 85, # Количество возможных вариантов действия\n",
        "    'rewardDecay' : 0.95, # Коэффициент изменения награждения\n",
        "    'opt_rate' : 0.001 # rate оптимизатора    \n",
        "}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fIvwBe0OZsG"
      },
      "source": [
        "# Функция создания модели\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Reshape((80, 80, 1), input_shape=(parameters['observation_size'],)))\n",
        "  model.add(Conv2D(16, 8, strides=(4, 4), activation='relu'))\n",
        "  model.add(Conv2D(32, 4, strides=(2, 2),activation='relu'))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(32, activation='relu'))\n",
        "  model.add(Dense(parameters['action_size'], activation='softmax'))\n",
        "  opt = Adam(lr=parameters['opt_rate'])\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
        "  return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgShlx-PP0mJ"
      },
      "source": [
        "# Функция пересчета вознаграждения\n",
        "def processRewards(rewardList): # 14 агентов на входе\n",
        "  rewardDecayed = np.zeros_like(rewardList,dtype=np.float32) # Создаем список из нулей с размерностью равной rewardList\n",
        "  tmp = 0 # Переменная для временного значения\n",
        "  for t in reversed(range(0, rewardList.size)): \n",
        "    if rewardList[t] == -1: \n",
        "      for i in range(1, 7): \n",
        "        rewardList[t-i] = -0.999          \n",
        "    if rewardList[t] == 1: \n",
        "      for i in range(1, 17): \n",
        "        rewardList[t-i] = 0.999\n",
        "    if rewardList[t] != 0: \n",
        "      tmp = 0 \n",
        "    tmp = tmp * parameters['rewardDecay'] + rewardList[t] \n",
        "    rewardDecayed[t] = tmp \n",
        "  return rewardDecayed "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYRwjn8ySGO3"
      },
      "source": [
        "# Функция преобразования фрейма\n",
        "def preprocessFrames(Frame): \n",
        "  Frame = Frame[35:195] \n",
        "  Frame = Frame[::2,::2, 0] \n",
        "  Frame[Frame==144] = 0 \n",
        "  Frame[Frame==109] = 0 \n",
        "  Frame[Frame != 0] = 1 \n",
        "  return Frame.astype(np.float).ravel() # функция вернет разницу между кадрами в виде одномерного вектора"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmypRcSzCMHG"
      },
      "source": [
        "## Создание модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyKWO5wk0Kwp"
      },
      "source": [
        "env = []\n",
        "observation = env.reset() \n",
        "lastObservation = None \n",
        "model = create_model()\n",
        "\n",
        "# Создадим пустые массивы под выборки\n",
        "gradients = [] # Сюда будем записывать награждение за правильный шаг и противоположное значение за неправильный на текущем шаге\n",
        "states = [] # Массив состояний в течение одного эпизода\n",
        "rewards = [] # Массив наград в течение одного эпизода\n",
        "predictions = [] # Массив, в который будем записывать получаемые на выходе модели значения\n",
        "num_episode = 101000 # Номер эпизода\n",
        "\n",
        "# СТАТИСТИКА\n",
        "frameCount = 0 # Количество фреймов в эпизоде\n",
        "score = 0 # Результат текущего эпизода\n",
        "losses = [] # Массив, куда будем записывать ошибки\n",
        "frameCounts = [] # Массив, куда будем записывать фреймы в эпизоде\n",
        "averages = [] # Массив для среднего значения результата эпизода\n",
        "scores = [] # Массив для сохранения результатов эпизода\n",
        "\n",
        "model.load_weights(parameters['path']+'model'+str(num_episode)+'.h5')\n",
        "while True:\n",
        "  # Получаем разницу кадров  \n",
        "  prepFrame = preprocessFrames(observation) \n",
        "  diffFrame = prepFrame - lastObservation if lastObservation is not None else np.zeros(parameters['observation_size']) \n",
        "  lastObservation = prepFrame \n",
        "\n",
        "  # Определяем наше действие\n",
        "  state = diffFrame.reshape([1, diffFrame.shape[0]]) \n",
        "  pred = model.predict(state, batch_size=1).flatten() \n",
        "  predictions.append(pred) \n",
        "  action = np.random.choice(parameters['action_size'], 1, p=pred)[0] \n",
        "\n",
        "  # Выполняем действие\n",
        "  observation, reward, done, info = env.step(3) \n",
        "  y = np.zeros(parameters['action_size']) \n",
        "  y[action] = 1 \n",
        "  gradients.append(np.array(y).astype('float32') - pred) \n",
        "  \n",
        "\n",
        "  states.append(diffFrame) \n",
        "  rewards.append(reward) \n",
        "  score += reward \n",
        "  frameCount+= 1 \n",
        "\n",
        "  if done: # Если завершился эпизод\n",
        "    num_episode += 1 \n",
        "\n",
        "    # Обучаем модель на данных \n",
        "    gradients = np.vstack(gradients)\n",
        "    rewards = np.vstack(rewards) \n",
        "    rewards = processRewards(rewards) \n",
        "    rewards = rewards / np.std(rewards - np.mean(rewards)) \n",
        "    gradients *= rewards \n",
        "    \n",
        "    x_train = np.squeeze(np.vstack([states])) \n",
        "    y_train = predictions + parameters['opt_rate'] * np.squeeze(np.vstack([gradients])) \n",
        "    loss = model.train_on_batch(x_train, y_train) \n",
        "    \n",
        "    losses.append(loss) # Добавялем ошибки\n",
        "    frameCounts.append(frameCount) \n",
        "    scores.append(score) \n",
        "    observation = env.reset() \n",
        "    average = sum(scores)/len(scores) \n",
        "    averages.append(average) \n",
        "\n",
        "    # Выводим статистику за эпизод\n",
        "    print('Эпизод №: %d - Коэффициэнт: %d (%.2f) - Среднее значение: %.2f - Предикт: [%.3f, %.3f] - Ошибка: %.4f - Фреймов: %d (%.1f)' \\\n",
        "          % (num_episode, score, np.mean(np.array(scores)[-20:]), (average), pred[0], pred[1], loss, frameCount, np.mean(np.array(frameCounts)[-20:])))\n",
        "    # Обнуляем значения\n",
        "    frameCount = 0 \n",
        "    score = 0\n",
        "    states = []\n",
        "    predictions = []\n",
        "    rewards = []\n",
        "    gradients = []\n",
        "\n",
        "    # Каждый 200-ый эпизод сохраняем модель\n",
        "    if num_episode > 1 and num_episode % 100 == 0:\n",
        "      model.save_weights(parameters['path']+'model'+str(num_episode)+'.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afmyEkeXN0kd"
      },
      "source": [
        "PS: модель считает комбинации на вхождение в норму в размере 9,5 рабочих часов"
      ]
    }
  ]
}